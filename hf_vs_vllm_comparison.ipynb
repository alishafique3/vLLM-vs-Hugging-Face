{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11f1f2d-db99-4d5c-b1f9-34c2b4847a2b",
      "metadata": {
        "tags": [],
        "id": "d11f1f2d-db99-4d5c-b1f9-34c2b4847a2b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"<hf-api-key>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b43d863d-381a-4bc0-bb2b-874d879bd7e0",
      "metadata": {
        "tags": [],
        "id": "b43d863d-381a-4bc0-bb2b-874d879bd7e0",
        "outputId": "2871794e-be06-4b16-da7c-7673f46776b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 05-20 11:09:31 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-20 11:09:33,636\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 05-20 11:09:43 [config.py:717] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
            "INFO 05-20 11:09:43 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "INFO 05-20 11:09:45 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
            "WARNING 05-20 11:09:45 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6c2a921540>\n",
            "INFO 05-20 11:09:46 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 05-20 11:09:46 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
            "WARNING 05-20 11:09:46 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 05-20 11:09:46 [gpu_model_runner.py:1329] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
            "INFO 05-20 11:09:46 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
            "INFO 05-20 11:09:47 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.2-3B-Instruct: 0.888471 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.28s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.26it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 05-20 11:09:49 [loader.py:458] Loading weights took 1.76 seconds\n",
            "INFO 05-20 11:09:49 [gpu_model_runner.py:1347] Model loading took 6.0160 GiB and 3.119919 seconds\n",
            "INFO 05-20 11:09:59 [backends.py:420] Using cache directory: /home/jupyter/.cache/vllm/torch_compile_cache/bdc21dc8c4/rank_0_0 for vLLM's torch.compile\n",
            "INFO 05-20 11:09:59 [backends.py:430] Dynamo bytecode transform time: 9.39 s\n",
            "INFO 05-20 11:10:05 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.119 s\n",
            "INFO 05-20 11:10:07 [monitor.py:33] torch.compile takes 9.39 s in total\n",
            "INFO 05-20 11:10:08 [kv_cache_utils.py:634] GPU KV cache size: 114,816 tokens\n",
            "INFO 05-20 11:10:08 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 56.06x\n",
            "INFO 05-20 11:10:33 [gpu_model_runner.py:1686] Graph capturing finished in 25 secs, took 0.44 GiB\n",
            "INFO 05-20 11:10:33 [core.py:159] init engine (profile, create kv cache, warmup model) took 43.62 seconds\n",
            "INFO 05-20 11:10:33 [core_client.py:439] Core engine process 0 ready.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s, est. speed input: 83.80 toks/s, output: 94.70 toks/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Batch Outputs ---\n",
            "\n",
            "Prompt 1: What is Thailand's national food symbol?\n",
            "Response 1: assistant\n",
            "\n",
            "Based on its rich culinary culture, Thailand's national food symbol is the Congee, also known as tom yum na kanom jeen, a type of coconut milk-based soup, but another symbol is sticky rice or glutinous rice. However, a more commonly accepted symbol is the Mango, especially as in ' sticky rice and mango', as featured on Thailand's architectural and national emblems.\n",
            "\n",
            "Prompt 2: What is the capital of Norway?\n",
            "Response 2: assistant\n",
            "\n",
            "The capital of Norway is Oslo. It is the country's largest city and is situated on the southern part of the country, along the Vimmerbumerelvi river.\n",
            "\n",
            "Prompt 3: Who invented the telescope?\n",
            "Response 3: assistant\n",
            "\n",
            "The invention of the telescope is credited to Hans Lippershey, a Dutch spectacle maker, and Zacharias Janssen, another Dutch spectacle maker, who respectively received patents for their designs in 1608. However, it was Galileo Galilei, an Italian astronomer, who improved and popularized the telescope by creating his own refracting telescope in 1609.\n",
            "\n",
            "Prompt 4: What is the chemical symbol for gold?\n",
            "Response 4: assistant\n",
            "\n",
            "The chemical symbol for gold is Au, derived from the Latin word \"Aurum\".\n",
            "\n",
            "⏱ Inference Time (batch of 4): 2.40 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize tokenizer separately to apply chat template\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Just in case\n",
        "\n",
        "# Define a batch of prompts (32 in total)\n",
        "user_questions = [\n",
        "    \"What is Thailand's national food symbol?\",\n",
        "    \"What is the capital of Norway?\",\n",
        "    \"Who invented the telescope?\",\n",
        "    \"What is the chemical symbol for gold?\",\n",
        "    # \"Tell me a fun fact about dolphins.\",\n",
        "    # \"How many continents are there on Earth?\",\n",
        "    # \"What is the speed of light in vacuum?\",\n",
        "    # \"Who wrote the play 'Hamlet'?\",\n",
        "    #\"What causes tides in the ocean?\",\n",
        "    #\"Name the largest planet in our solar system.\",\n",
        "    #\"What language is primarily spoken in Brazil?\",\n",
        "    #\"What's the boiling point of water in Celsius?\",\n",
        "    #\"Who painted the Mona Lisa?\",\n",
        "    #\"What is the main ingredient in guacamole?\",\n",
        "    #\"Why do cats purr?\",\n",
        "    #\"What is the tallest mountain in the world?\",\n",
        "] * 2\n",
        "\n",
        "# Create structured chat messages\n",
        "messages_batch = [\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond in two sentences.\"},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    for question in user_questions\n",
        "]\n",
        "\n",
        "# Manually apply chat template to each prompt\n",
        "rendered_prompts = [\n",
        "    tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    for messages in messages_batch\n",
        "]\n",
        "\n",
        "# Define generation parameters\n",
        "sampling_params = SamplingParams(max_tokens=128)\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize vLLM\n",
        "llm = LLM(\n",
        "    model=MODEL_NAME,\n",
        "    max_model_len=2048,\n",
        ")\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Run batch generation\n",
        "outputs = llm.generate(rendered_prompts, sampling_params)\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()\n",
        "\n",
        "# Print outputs\n",
        "print(\"--- Batch Outputs ---\")\n",
        "for i, output in enumerate(outputs):\n",
        "    print(f\"\\nPrompt {i+1}: {user_questions[i]}\")\n",
        "    print(f\"Response {i+1}: {output.outputs[0].text.strip()}\")\n",
        "\n",
        "# Report inference time\n",
        "inference_time = end_time - start_time\n",
        "print(f\"\\n⏱ Inference Time (batch of {len(user_questions)}): {inference_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab37a8d-bd5c-4013-a602-a145495e57d8",
      "metadata": {
        "id": "dab37a8d-bd5c-4013-a602-a145495e57d8"
      },
      "source": [
        "| Batch Size | Inference Time (seconds) |\n",
        "|------------|--------------------------|\n",
        "| 32         | 3.38                     |\n",
        "| 16         | 3.01                     |\n",
        "| 8          | 2.51                     |\n",
        "| 4          | 2.32                     |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe7216a0-75a9-4faf-b5ba-8e12d58c0235",
      "metadata": {
        "tags": [],
        "id": "fe7216a0-75a9-4faf-b5ba-8e12d58c0235",
        "outputId": "c57ea27c-f4cc-414d-819f-1efff6dd8240"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Batch Outputs ---\n",
            "\n",
            "Prompt 1: What is Thailand's national food symbol?\n",
            "Response 1: assistant\n",
            "\n",
            "The national food symbol of Thailand is the \"khao niew\" or sticky rice, which is typically served with most Thai meals. It is a staple food in Thai cuisine and is often served alongside dishes such as curries, soups, and grilled meats.\n",
            "\n",
            "Prompt 2: What is the capital of Norway?\n",
            "Response 2: assistant\n",
            "\n",
            "The capital of Norway is Oslo. It is located in the southeastern part of the country and is known for its cultural and historical landmarks, such as the Opera House and the Viking Ship Museum.\n",
            "\n",
            "Prompt 3: Who invented the telescope?\n",
            "Response 3: assistant\n",
            "\n",
            "The invention of the telescope is credited to Hans Lippershey, a Dutch spectacle maker, who patented the first practical refracting telescope in 1608. However, Galileo Galilei is often credited with the first practical refracting telescope in 1609, as he improved upon Lippershey's design and made significant contributions to the field of astronomy.\n",
            "\n",
            "Prompt 4: What is the chemical symbol for gold?\n",
            "Response 4: assistant\n",
            "\n",
            "The chemical symbol for gold is Au, which is derived from the Latin word for gold, \"aurum.\" This symbol is widely used in chemistry and other fields to represent the element gold.\n",
            "\n",
            "⏱ Inference Time (batch of 5): 5.64 seconds\n",
            "📊 Peak GPU Memory: 12405.74 MB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    padding_side=\"left\",\n",
        "    truncation_side=\"left\"\n",
        ")\n",
        "\n",
        "# Set pad_token (LLaMA doesn't define one by default)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define batch of user prompts\n",
        "user_questions = [\n",
        "    \"What is Thailand's national food symbol?\",\n",
        "    \"What is the capital of Norway?\",\n",
        "    \"Who invented the telescope?\",\n",
        "    \"What is the chemical symbol for gold?\",\n",
        "    # \"Tell me a fun fact about dolphins.\",\n",
        "    # \"How many continents are there on Earth?\",\n",
        "    # \"What is the speed of light in vacuum?\",\n",
        "    # \"Who wrote the play 'Hamlet'?\",\n",
        "    #\"What causes tides in the ocean?\",\n",
        "    #\"Name the largest planet in our solar system.\",\n",
        "    #\"What language is primarily spoken in Brazil?\",\n",
        "    #\"What's the boiling point of water in Celsius?\",\n",
        "    #\"Who painted the Mona Lisa?\",\n",
        "    #\"What is the main ingredient in guacamole?\",\n",
        "    #\"Why do cats purr?\",\n",
        "    #\"What is the tallest mountain in the world?\",\n",
        "] * 2\n",
        "\n",
        "# Format messages in chat format\n",
        "messages_batch = [\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond in two sentences.\"},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    for question in user_questions\n",
        "]\n",
        "\n",
        "# Tokenize using chat template with padding and truncation\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages_batch,\n",
        "    tokenize=True,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=2048\n",
        ").to(model.device)\n",
        "\n",
        "# Clear previous GPU stats\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Generate responses\n",
        "input_length = input_ids.shape[1]\n",
        "generated_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=128,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()\n",
        "\n",
        "# Decode outputs\n",
        "decoded_outputs = tokenizer.batch_decode(\n",
        "    generated_ids[:, input_length:],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# Print outputs\n",
        "print(\"--- Batch Outputs ---\")\n",
        "for i, output in enumerate(decoded_outputs):\n",
        "    print(f\"\\nPrompt {i+1}: {user_questions[i]}\")\n",
        "    print(f\"Response {i+1}: {output.strip()}\")\n",
        "\n",
        "# Report performance\n",
        "inference_time = end_time - start_time\n",
        "peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # in MB\n",
        "\n",
        "print(f\"\\n⏱ Inference Time (batch of 5): {inference_time:.2f} seconds\")\n",
        "print(f\"📊 Peak GPU Memory: {peak_memory:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5e544f-479b-4eb7-9e70-8eac795bef64",
      "metadata": {
        "id": "1d5e544f-479b-4eb7-9e70-8eac795bef64"
      },
      "source": [
        "| Batch Size | Inference Time (seconds) |\n",
        "|------------|--------------------------|\n",
        "| 32         | 12.90                    |\n",
        "| 16         | 9.50                     |\n",
        "| 8          | 7.24                     |\n",
        "| 4          | 5.32                     |"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uhgvZfGfLKmO"
      },
      "id": "uhgvZfGfLKmO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "vllm",
      "name": "workbench-notebooks.m129",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
    },
    "kernelspec": {
      "display_name": "Python (vllm) (Local)",
      "language": "python",
      "name": "vllm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}